---
title: "eDNAModel: Updated Workflow with Phyloseq"
author: "Marwah Soliman, Bert van der Veen"
date: "`r Sys.Date()`"
output:
  rmarkdown::html_vignette:
    toc: true
    toc_depth: 3
    number_sections: true
vignette: >
  %\VignetteIndexEntry{eDNAModel: Updated Workflow with Phyloseq}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup1, include=FALSE}
library(phyloseq)
library(Matrix)
library(TMB)
```
---
```{r example, eval=TRUE}
# Create OTU table
# ----------------------------
# LOAD REQUIRED PACKAGE
# ----------------------------
library(phyloseq)
# ----------------------------
######################################## Dynamic code #############################################
library(phyloseq)
library(eDNAModel)
library(dplyr)
library(tidyr)
library(tibble)
library(glmmTMB)

col_map <- list(
  sampletype = "sampletype",
  location   = "location",
  treatment  = "treatment",
  sample     = "sample",
  samplerep  = "samplerep",
  atoll      = "atoll",
  exblank    = "exblank",
  samp.blank = "samp.blank"
)

physeq_path <- system.file("extdata", "longdataexample.RDS", package = "eDNAModel")
physeq <- readRDS(physeq_path)

out_data <- prepare_long_data(
  physeq_obj        = physeq,
  min_species_sum   = 50,
  sampletype_keep   = "biologicalsample",
  col_vars          = col_map
)


long_df <- out_data$long_df


# Subset 50 OTUs
otu_levels <- levels(long_df$i)[1:50]
otu_subset <- subset(long_df, i %in% otu_levels)
otu_subset <- droplevels(otu_subset)
otu_subset$i<- as.factor(otu_subset$i)
otu_subset$i <- relevel(otu_subset$i, ref = "OTU1005")

# Step 1: Make sure "0" is not in the data
otu_subset <- otu_subset %>%
  filter(treatment != "0")

# Step 2: Drop unused levels
otu_subset$treatment <- droplevels(factor(otu_subset$treatment))

# Step 3: Confirm
levels(otu_subset$treatment)



# Fit model
out <- simulate_glm_burnin_iterations(
            data_glm = otu_subset,
            poisson_formula = y ~ (1|Site) + (1 | Sample) + (1 | Replicate) +treatment*i,
            binomial_formula = z_sim ~ (1|Site)+ treatment*i,
            num_iterations   = 100,
            burn_in          = 50
        )

     #plot the interaction, first extract the poisson and plot , then extract from binomial and plot

     poisson_fixed_all <- extract_fixed(out$poisson_models, "poisson")
     interaction_terms <- poisson_fixed_all %>%
       filter(grepl("treatment.*:i", term))
     
     any(grepl("OTU1005", interaction_terms$term))  # SHOULD be FALSE
    
     
     binomial_fixed_all <- extract_fixed(out$binomial_models, "binomial")
     interaction_terms <- binomial_fixed_all %>%
       filter(grepl("treatment.*:i", term))


##for the plot apply the following once for poisson and then onece for binomial.
     
     library(dplyr)
     library(ggplot2)
     
     # Step 1: Extract OTU from term
     interaction_terms_clean <- interaction_terms %>%
       mutate(OTU = gsub("treatmentRats:i", "", term))
     
     # Step 2: Compute mean and SD of estimates for each OTU
     otu_estimates <- interaction_terms_clean %>%
       group_by(OTU) %>%
       summarise(
         mean_estimate = mean(Estimate, na.rm = TRUE),
         sd_estimate   = sd(Estimate, na.rm = TRUE)
       ) %>%                                       
       arrange(mean_estimate) %>%
       mutate(OTU = factor(OTU, levels = OTU))  # Ensure ordered factor for plotting
     
     # Step 3: Caterpillar plot with error bars (SD)
     ggplot(otu_estimates, aes(x = OTU, y = mean_estimate)) +
       geom_point() +
       geom_errorbar(aes(ymin = mean_estimate - 1.96*sd_estimate,
                         ymax = mean_estimate + 1.96*sd_estimate),
                     width = 0.2) +
       geom_hline(yintercept = 0, linetype = "dashed") +
       coord_flip() +
       labs(
         title = "Caterpillar Plot: treatmentRats × OTU Interaction Effects",
         x = "OTU",
         y = "Mean Estimate ± SD"
       ) +
       theme_minimal()


# ---------- Step 4: Extract predictions ----------
# Occupancy predictions (psi)
psi_mat <- do.call(cbind, lapply(out$binomial_models, function(model) {
  predict(model, type = "response", newdata = otu_subset)
}))

# Abundance predictions (lambda)
lambda_mat <- do.call(cbind, lapply(out$poisson_models, function(model) {
  predict(model, type = "response", newdata = otu_subset)
}))

# Detection probability
p_detect_mat <- 1 - exp(-lambda_mat)

# ---------- Step 5: Summarize per OTU ----------
otu_summary <- data.frame(
  OTU           = otu_subset$i,
  psi_mean      = rowMeans(psi_mat),
  psi_se        = apply(psi_mat, 1, sd),
  psi_lwr       = apply(psi_mat, 1, quantile, probs = 0.025),
  psi_upr       = apply(psi_mat, 1, quantile, probs = 0.975),
  lambda_mean   = rowMeans(lambda_mat),
  lambda_se     = apply(lambda_mat, 1, sd),
  p_detect_mean = rowMeans(p_detect_mat),
  p_detect_se   = apply(p_detect_mat, 1, sd),
  p_detect_lwr  = apply(p_detect_mat, 1, quantile, probs = 0.025),
  p_detect_upr  = apply(p_detect_mat, 1, quantile, probs = 0.975)
) %>%
  group_by(OTU) %>%
  summarise(
    psi_mean        = mean(psi_mean),
    psi_se          = mean(psi_se),
    psi_lwr         = mean(psi_lwr),
    psi_upr         = mean(psi_upr),
    lambda_mean     = mean(lambda_mean),
    lambda_se       = mean(lambda_se),
    p_detect_mean   = mean(p_detect_mean),
    p_detect_se     = mean(p_detect_se),
    p_detect_lwr    = mean(p_detect_lwr),
    p_detect_upr    = mean(p_detect_upr),
    .groups = "drop"
  )

# ---------- View Results ----------
print(otu_summary)

library(ggplot2)
library(dplyr)

# Assuming otu_summary is already loaded in your environment
# If not, read or create it accordingly

# Ensure OTUs are ordered by psi_mean for plotting
otu_summary_ordered <- otu_summary %>%
  arrange(psi_mean) %>%
  mutate(OTU = factor(OTU, levels = OTU))

# Caterpillar plot
ggplot(otu_summary_ordered, aes(x = OTU, y = psi_mean)) +
  geom_point() +
  geom_errorbar(aes(ymin = psi_lwr, ymax = psi_upr), width = 0.2) +
  geom_hline(yintercept = 0, linetype = "dashed") +
  coord_flip() +
  labs(
    title = "Caterpillar Plot of OTU Occupancy Probabilities (ψ)",
    x = "OTU",
    y = "Mean ψ (with 95% CI)"
  ) +
  theme_minimal()


otu_predictions <- otu_subset[, c("i", "treatment")] %>%
       mutate(
         psi_mean      = rowMeans(psi_mat),
         psi_se        = apply(psi_mat, 1, sd),
         lambda_mean   = rowMeans(lambda_mat),
         lambda_se     = apply(lambda_mat, 1, sd),
         p_detect_mean = rowMeans(p_detect_mat),
         p_detect_se   = apply(p_detect_mat, 1, sd)
       )
     
     otu_treatment_summary <- otu_predictions %>%
       group_by(i, treatment) %>%
       summarise(
         psi_mean        = mean(psi_mean),
         psi_se          = mean(psi_se),
         lambda_mean     = mean(lambda_mean),
         lambda_se       = mean(lambda_se),
         p_detect_mean   = mean(p_detect_mean),
         p_detect_se     = mean(p_detect_se),
         .groups = "drop"
       )
     
     
     library(ggplot2)
     library(dplyr)
     
     # Assuming `otu_treatment_summary` is your data frame
     
     # Reorder OTUs for better visual clarity
     otu_treatment_summary <- otu_treatment_summary %>%
       mutate(i = factor(i, levels = unique(i[order(psi_mean)])))
     
     # Plot psi_mean with standard error bars
     ggplot(otu_treatment_summary, aes(x = psi_mean, y = i, color = treatment)) +
       geom_point() +
       geom_errorbarh(aes(xmin = psi_mean - psi_se, xmax = psi_mean + psi_se), height = 0.2) +
       labs(
         x = "Psi Mean (Occupancy Probability)",
         y = "OTU",
         title = "Caterpillar Plot for Psi Estimates by OTU and Treatment"
       ) +
       theme_minimal() +
       theme(
         legend.position = "top",
         axis.text.y = element_text(size = 7)
       )
    
     library(ggplot2)
     library(dplyr)
     
     # Reorder OTUs by detection probability for clarity
     otu_treatment_summary <- otu_treatment_summary %>%
       mutate(i = factor(i, levels = unique(i[order(p_detect_mean)])))
     
     # Plot p_detect_mean with standard error bars
     ggplot(otu_treatment_summary, aes(x = p_detect_mean, y = i, color = treatment)) +
       geom_point() +
       geom_errorbarh(aes(xmin = p_detect_mean - p_detect_se, xmax = p_detect_mean + p_detect_se), height = 0.2) +
       labs(
         x = "Detection Probability",
         y = "OTU",
         title = "Caterpillar Plot for Detection Probability by OTU and Treatment"
       ) +
       theme_minimal() +
       theme(
         legend.position = "top",
         axis.text.y = element_text(size = 7)
       )
     
########################################heat map##########################################
#####################Heat map######################################
     library(dplyr)
     
     otu_pred_df <- otu_subset %>%
       dplyr::select(i, Site) %>%
       mutate(
         psi = rowMeans(psi_mat),
         p_detect = rowMeans(p_detect_mat),
         detected = p_detect == 1,
         display_psi = ifelse(detected, 1, psi)
       )
     
     otu_pred_df <- otu_pred_df %>%
       mutate(i = factor(i, levels = unique(i[order(-psi)])))
     
     # Step 3: Plot heatmap with Site on x-axis and OTU on y-axis
     ggplot(otu_pred_df, aes(x = Site, y = i)) +
       geom_tile(data = filter(otu_pred_df, detected), fill = "grey70") +
       geom_tile(data = filter(otu_pred_df, !detected), aes(fill = display_psi)) +
       scale_fill_gradient(low = "blue", high = "orange", name = "Occupancy\nProbability (ψ)") +
       labs(
         x = "Site",
         y = "OTU",
         title = "Predicted Probability of Species Occupancy by OTU and Site\nGrey = Detected (p_detect = 1)"
       ) +
       theme_minimal() +
       theme(
         axis.text.x = element_text(angle = 45, hjust = 1),
         axis.text.y = element_text(size = 6),
         legend.position = "right"
       )
     

############################### model comparison#########################################

##For example 

out1 <- simulate_glm_burnin_iterations(
  data_glm = otu_subset,
  poisson_formula  = y ~ (1|Site) + i*treatment + (1 | Sample) + (1 | Replicate),
  binomial_formula = z_sim ~ (1|Site) + i*treatment,
  num_iterations   = 100,
  burn_in          = 50
)

out2 <- simulate_glm_burnin_iterations(
  data_glm = otu_subset,
  poisson_formula  = y ~ (1|Site) + i*treatment,
  binomial_formula = z_sim ~ (1|Site) + i*treatment,
  num_iterations   = 100,
  burn_in          = 50
)



extract_model_metrics <- function(model_list) {
  metrics_df <- do.call(rbind, lapply(model_list, function(mod) {
    data.frame(
      AIC = AIC(mod),
      BIC = BIC(mod),
      NegLogLik = -logLik(mod)[1]
    )
  }))
  return(metrics_df)
}

# Poisson model metrics
poisson_metrics_out1 <- extract_model_metrics(out1$poisson_models)
poisson_metrics_out2 <- extract_model_metrics(out2$poisson_models)

# Binomial model metrics
binomial_metrics_out1 <- extract_model_metrics(out1$binomial_models)
binomial_metrics_out2 <- extract_model_metrics(out2$binomial_models)


summary_metrics <- data.frame(
  Model = c("Poisson_Out1", "Poisson_Out2", "Binomial_Out1", "Binomial_Out2"),
  AIC   = c(mean(poisson_metrics_out1$AIC),
            mean(poisson_metrics_out2$AIC),
            mean(binomial_metrics_out1$AIC),
            mean(binomial_metrics_out2$AIC)),
  BIC   = c(mean(poisson_metrics_out1$BIC),
            mean(poisson_metrics_out2$BIC),
            mean(binomial_metrics_out1$BIC),
            mean(binomial_metrics_out2$BIC)),
  NegLogLik = c(mean(poisson_metrics_out1$NegLogLik),
                mean(poisson_metrics_out2$NegLogLik),
                mean(binomial_metrics_out1$NegLogLik),
                mean(binomial_metrics_out2$NegLogLik))
)

print(summary_metrics)
############################ model comparison model as whole ############################

# Helper function to extract metrics
extract_model_metrics <- function(model_list) {
  metrics_df <- do.call(rbind, lapply(model_list, function(mod) {
    data.frame(
      AIC = AIC(mod),
      BIC = BIC(mod),
      NegLogLik = -logLik(mod)[1]
    )
  }))
  return(metrics_df)
}

# Extract metrics for out1
poisson_metrics_out1 <- extract_model_metrics(out1$poisson_models)
binomial_metrics_out1 <- extract_model_metrics(out1$binomial_models)

# Extract metrics for out2
poisson_metrics_out2 <- extract_model_metrics(out2$poisson_models)
binomial_metrics_out2 <- extract_model_metrics(out2$binomial_models)

# Sum metrics across both model types
overall_metrics <- data.frame(
  Model = c("out1", "out2"),
  Total_AIC = c(
    sum(poisson_metrics_out1$AIC) + sum(binomial_metrics_out1$AIC),
    sum(poisson_metrics_out2$AIC) + sum(binomial_metrics_out2$AIC)
  ),
  Total_BIC = c(
    sum(poisson_metrics_out1$BIC) + sum(binomial_metrics_out1$BIC),
    sum(poisson_metrics_out2$BIC) + sum(binomial_metrics_out2$BIC)
  ),
  Total_NegLogLik = c(
    sum(poisson_metrics_out1$NegLogLik) + sum(binomial_metrics_out1$NegLogLik),
    sum(poisson_metrics_out2$NegLogLik) + sum(binomial_metrics_out2$NegLogLik)
  )
)

# View the comparison
print(overall_metrics)




#######################################fit model#####################################################

library(glmmTMB)
library(dplyr)

# ---- Function ----
simulate_glm_burnin_iterations <- function(data_glm,
                                           poisson_formula = y ~ 1,
                                           binomial_formula = z_sim ~ 1,
                                           num_iterations = 300,
                                           burn_in = 100) {
  
  # initialise latent occupancy state
  data_glm$z_sim <- ifelse(data_glm$y > 0, 1,
                           rbinom(nrow(data_glm), 1, 0.5))
  
  # storage for models
  poisson_models  <- list()
  binomial_models <- list()
  Z_list          <- list()
  
  # iteration loop (only Z + models)
  for (iter in 1:num_iterations) {
    
    # subset for Poisson (only z=1 rows)
    Q <- data_glm[data_glm$z_sim == 1, ]
    
    # fit Poisson (abundance)
    model_poisson <- glmmTMB(poisson_formula,
                             family = poisson,
                             data = Q)
    
    # fit Binomial (occupancy)
    model_binomial <- glmmTMB(binomial_formula,
                              family = binomial,
                              data = data_glm)
    
    # predictions
    lambda_i <- predict(model_poisson, type = "response",
                        newdata = data_glm)
    P_i      <- predict(model_binomial, type = "response",
                        newdata = data_glm)
    
    # update latent occupancy Z
    prob_Z1_given_y0 <- P_i * exp(-lambda_i) /
      (P_i * exp(-lambda_i) + (1 - P_i))
    prob_Z1_given_y0 <- pmin(pmax(prob_Z1_given_y0,
                                  1e-6), 1 - 1e-6)
    
    data_glm$z_sim[data_glm$y == 0] <-
      rbinom(sum(data_glm$y == 0), 1,
             prob_Z1_given_y0[data_glm$y == 0])
    
    # store models & Z only after burn-in
    if (iter > burn_in) {
      poisson_models[[iter - burn_in]]  <- model_poisson
      binomial_models[[iter - burn_in]] <- model_binomial
      Z_list[[iter - burn_in]]          <- data_glm$z_sim
    }
  }
  
  return(list(
    poisson_models  = poisson_models,
    binomial_models = binomial_models,
    Z_list          = Z_list
  ))
}

# ---- Example run ----
out <- simulate_glm_burnin_iterations(
  data_glm=long_df,
  poisson_formula  = y ~ Site + (1|Sample) + (1|Replicate),
  binomial_formula = z_sim ~ Site,
  num_iterations   = 100,
  burn_in          = 50
)

# --- outside loop: extract summaries ---

# fixed effects extractor
extract_fixed <- function(model_list, model_name) {
  do.call(rbind, lapply(seq_along(model_list), function(i) {
    sm <- as.data.frame(summary(model_list[[i]])$coefficients$cond)
    sm$term <- rownames(sm)
    sm$iter <- i
    sm$model <- model_name
    sm
  }))
}

# random effect extractor
extract_random <- function(model_list, model_name) {
  do.call(rbind, lapply(seq_along(model_list), function(i) {
    vc <- VarCorr(model_list[[i]])
    if (length(vc$cond) > 0) {
      data.frame(
        group = names(vc$cond),
        var   = sapply(vc$cond, function(x) attr(x, "stddev")^2),
        iter  = i,
        model = model_name,
        stringsAsFactors = FALSE
      )
    } else {
      data.frame(group = NA, var = NA, iter = i, model = model_name)
    }
  }))
}

# extract all fixed & random effects
poisson_fixed_all  <- extract_fixed(out$poisson_models, "poisson")
binomial_fixed_all <- extract_fixed(out$binomial_models, "binomial")

poisson_random_all  <- extract_random(out$poisson_models, "poisson")
binomial_random_all <- extract_random(out$binomial_models, "binomial")

# summarise averages
summarise_fixed <- function(df) {
  df %>%
    group_by(term) %>%
    summarise(
      estimate_mean = mean(Estimate, na.rm = TRUE),
      se_mean       = mean(`Std. Error`, na.rm = TRUE),
      z_value       = estimate_mean / se_mean,
      p_value       = 2 * (1 - pnorm(abs(z_value))),
      .groups = "drop"
    )
}

poisson_fixed_summary  <- summarise_fixed(poisson_fixed_all)
binomial_fixed_summary <- summarise_fixed(binomial_fixed_all)

poisson_random_summary <- poisson_random_all %>%
  group_by(group) %>%
  summarise(mean_var = mean(var, na.rm = TRUE), .groups = "drop")

binomial_random_summary <- binomial_random_all %>%
  group_by(group) %>%
  summarise(mean_var = mean(var, na.rm = TRUE), .groups = "drop")

# final results
results <- list(
  poisson_fixed_summary   = poisson_fixed_summary,
  binomial_fixed_summary  = binomial_fixed_summary,
  poisson_random_summary  = poisson_random_summary,
  binomial_random_summary = binomial_random_summary,
  Z                       = out$Z_list
)

results

results$poisson_fixed_summary

results$binomial_fixed_summary

results$poisson_random_summary

results$binomial_random_summary
```
---
## Fitted Values

```{r fitted_summary, echo=FALSE}
fitted_vals <- fitted_TMB(model)

summary(fitted_vals$fitted_abundance)
summary(fitted_vals$fitted_occupancy)

```
---

##Residuals

```{r run_resid, eval = TRUE}
# Residuals
# Make sure compute_residuals_TMB is sourced beforehand
residuals <- compute_residuals_TMB(model, y = y, X = X, type = "pearson")

# Display a preview
cat("✅ Abundance residuals (first few rows):\n")
print(head(residuals$abundance_residuals))

cat("\n✅ Occupancy residuals (first few rows):\n")
print(head(residuals$occupancy_residuals))

hist(residuals$abundance_residuals, main = "Abundance Residuals", col = "lightblue")
hist(residuals$occupancy_residuals, main = "Occupancy Residuals", col = "lightgreen")

```
---

## Predictions

```{r run-pred, eval = TRUE}

# Create newdata with unique Site levels for fixed-effect prediction
newdata <- data.frame(Site = factor(levels(X$Site), levels = levels(X$Site)))

# Predict abundance on the response scale with CI
pred_abund <- predict_TMB(
  model = model,
  newX = newdata,
  formula = ~ Site,  # only fixed effect
  which = "abundance", 
  type = "response",
  se = TRUE
)
pred_occ <- predict_TMB(model, newX = newdata, formula = ~ Site, which = "occupancy", type = "response")

head(pred_abund)
head(pred_occ)

cat("✅ Abundance predictions:\n")
print(pred_abund)
cat("✅ Occupancy predictions:\n")
print(pred_occ)
```
---


## Introduction

The eDNAModel package provides a robust pipeline for analyzing environmental DNA (eDNA) using hierarchical multispecies occupancy-abundance models implemented via Template Model Builder (TMB).

1.This updated vignette demonstrates how to:

2.Load a phyloseq object

3.Automatically prepare the data using fit.phyloseq()

4.Fit the model with minimal effort

5.Extract and visualize residuals and predictions

---

## Load a phyloseq object

We'll use an example `.RDS` file located in `inst/extdata/`.

```{r load-data, eval = TRUE}
physeq_path <- system.file("extdata", "longdataexample.RDS", package = "eDNAModel")
physeq <- readRDS(physeq_path)
```

---

## Run the Full TMB Pipeline
We now use the high-level wrapper fit.phyloseq() which handles:

Extracting OTU table

Filtering biological samples

Adding Site, Sample, Replicate columns

Running the TMB pipeline


```{r model, eval = TRUE}
model <- fit.phyloseq(
  phyloseq_obj = physeq,
  a.formula = ~ Site + diag(1 | Sample) + diag(1 | Replicate),
  o.formula = ~ Site,
  linko = 1,
  linka = 0,
  family = 1,
  control = list(startOptcontrol = list(maxit = 200),
  optControl = list(maxit = 10e3, sigma1 = 0.25), trace = TRUE)
)
```
---

##Model Summary

```{r sum1, echo=FALSE}
summary_out <- summary(model)
head(summary_out)
```

```{r exp, echo=FALSE}
attr(summary_out, "explanation")
```
---

## Fitted Values

```{r fitted_summary, echo=FALSE}
fitted_vals <- fitted_TMB(model)

summary(fitted_vals$fitted_abundance)
summary(fitted_vals$fitted_occupancy)

```
---

##Residuals

```{r run_resid, eval = TRUE}
# Residuals
residuals <- compute_residuals_TMB(model = model, y = y, X = X, type = "pearson")

hist(residuals$abundance_residuals, main = "Abundance Residuals", col = "lightblue")
hist(residuals$occupancy_residuals, main = "Occupancy Residuals", col = "lightgreen")

```
---

## Predictions

```{r run-pred, eval = TRUE}
newdata <- unique(X[, "Site", drop = FALSE])
newdata$Site <- factor(newdata$Site)

pred_abund <- predict_TMB(model, newX = newdata, formula = ~ Site, which = "abundance", type = "response")
pred_occ <- predict_TMB(model, newX = newdata, formula = ~ Site, which = "occupancy", type = "response")

head(pred_abund)
head(pred_occ)


```
---

##Check for Boundary Cases

```{r run-H, eval = TRUE}
# Check for extreme occupancy values
high_occ <- fitted_vals$fitted_occupancy >= 0.999
low_occ <- fitted_vals$fitted_occupancy <= 0.001

if (any(high_occ)) {
  warning(sum(high_occ), " entries with occupancy ~1. Possible overfitting.")
}
if (any(low_occ)) {
  warning(sum(low_occ), " entries with occupancy ~0. Check sparsity.")
}

```
---

##  Hessian Diagnostic

```{r run-Hes, eval = TRUE}
H <- tryCatch({
  model$TMBobj$he()
}, error = function(e) {
  message("ℹ️ Hessian calculation skipped (random effects present).")
  NULL
})

if (!is.null(H)) {
  eigenvalues <- eigen(H)$values
  if (any(eigenvalues <= 0)) {
    warning("⚠️ Non-positive definite Hessian detected. Consider simplifying the model.")
  } else {
    message("✅ Hessian is positive definite.")
  }
}

```
---

You can extract model output such as slopes, intercepts, and model fit metrics from `result`.

---

## Summary

This vignette demonstrated how to:

-- Use fit.phyloseq() for end-to-end modeling

-- Interpret and visualize model output

-- Compute residuals and predictions

--Perform diagnostics

Explore the `eDNAModel` documentation for further customization and model diagnostics.
